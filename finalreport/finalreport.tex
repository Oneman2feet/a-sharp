\documentclass{article}
\usepackage[top=1.2in, bottom=1in, left=2in, right=2in]{geometry}
\usepackage{enumerate, multicol}
\usepackage{amsmath}
\usepackage{parskip}
\usepackage{hyperref}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\code}{\texttt}

\begin{document}

\begin{titlepage}
\begin{center}

\textsc{\LARGE Cornell University}\\[1.5cm]

\textsc{\Large CS 4621 Practicum Final Report}\\[0.5cm]

% Title
\HRule \\[0.4cm]
{ \huge \bfseries A$\sharp$ -- Music Visualizer \\[0.4cm] }

\HRule \\[1.5cm]

% Group members
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
Shane \textsc{Moore} \\
\emph{swm85}
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
Zachary \textsc{Zimmerman} \\
\emph{ztz3}
\end{flushright}
\end{minipage}
\par\vspace{0.5cm}
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
Emre \textsc{Findik} \\
\emph{ef343}
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
Joseph \textsc{Vinegrad} \\
\emph{jav86}
\end{flushright}
\end{minipage}

\vfill

% Bottom of the page
{\large December 15, 2014}

\end{center}

\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Summary}

A$\sharp$ (A Sharp) is a music visualizer designed for meaningful sound information conveyance. While other visualizers have flashy and intricate animations that pleasantly accompany the music, they do not successfully convey, interperet, or even begin to replace the music. This is an enormous shortcoming of music visualizers --- current applications certainly do not meet the standards of a music visualizer in the true sense of the term. A$\sharp$ attemps to make strides toward filling that gap, with hopes that other visualizers may follow suit.

A$\sharp$ models a song using a single sphere mesh. The sphere is animated through sets of transformations based on comprehensive data analysis of the song's sound file. The application analyzes several important features of a song, including the overall key, beat event times, and the frequency amplitude spectrum at each instant of the song. The results of the song analysis determines the appearance of the sphere during the playback animation.

The source code for this project, as well as any extensions to it, can be found at \url{https://github.com/Oneman2feet/a-sharp/}.

\section{Conceptual Mapping}

In order to achieve an intuitive representation of the chosen song, the conceptual mapping of A$\sharp$'s visuals was designed with great care, as follows:

\begin{itemize}
    \item Overall volume determines object size
    \item Beats are shown as pulses in object size
    \item Overall pitch controlls object elevation (higher up means higher pitch)
    \item Sound complexity (distribution of frequencies) determines object shape
    \item Mood is conveyed by the color of the object
\end{itemize}

\section{Sound Analysis}

Sound analysis takes place as a pre-processing stage, before the visualization is run. The sound data is collected in the module \code{analysis.py}. Much of the waveform analysis is done with the help of the audio and music processing library \href{https://github.com/bmcfee/librosa/}{LibROSA}.

The data received from LibROSA includes:

\begin{itemize}
    \item The uncompressed waveform
    \item Separated harmonic and percussive waveforms
    \item Beat frames (list of time frames for which a beat event occurs)
    \item Mel Spectrogram (amplitude of each frequency bin over time)
    \item Chromagram (amplitude of each pitch-class over time)
\end{itemize}

From this initial data collection, more information about the song is gleaned. The frequency spectrum is truncated to a reasonable range and formatted as a displacement texture for the sphere. The spectrum is also condensed, via a weighted average, into a measure of the overall pitch at each instant. The overall key of the song is statistically predicted based on the prevalences of each pitch-class found in the chromagram, as discussed in \url{http://ismir2004.ismir.net/proceedings/p018-page-92-paper164.pdf}. 

The final conclusions of the analysis are then formatted into a python dictionary for use in the graphics module.

\section{Graphics Implementation}

The results from \code{analysis.py} are recieved by the module \code{graphics.py}. This module controls playback of the visualization with the help of the windowing and multimedia library \href{http://www.pyglet.org/}{Pyglet}.

The graphics module uses vertex and fragment shaders to set the sphere mesh's positional and color data. After initialization of the window and global variables and constants, the update function is scheduled to be called regularly by the Pyglet app. At each frame, the sphere's radius, position, color, and displacement map are calculated according to the current sound data. This information is passed into the shaders, \code{DispMapped.vert} and \code{DispMapped.frag}, which update the appearance of the sphere on screen.

The radius of the sphere is calculated as a sum of contributions
$$\text{radius} = \text{minimum radius} + \text{current volume} + \text{proximity to beat}$$
Where the proximity to the nearest beat is expressed as
$$\left( \frac{\text{time between beats}}{2} - \text{time since previous beat} \right)^2$$
in order to have the attack and decay of the beat appear as a pulse.

The vertical positioning of the sphere is affected by the overall pitch as determined in \code{analysis.py}. As the average pitch grows higher in frequency, the sphere tends to move upwards, while lower pitches cause it to descend. The vertical motion of the sphere is modeled after a damped harmonic oscillator, where the equilibrium location changes depending on the average pitch frequency.

color stuff

We attempted to represent the mood of the input song with the color of the sphere. We were partly inspired from the synesthesia condition in taking this approach, in which signals from a particular sense (smell, sight etc.) may invoke perceptions in another sense. It also appears that people without synesthesia are also able to associate sounds with colors although not perceiving them (Tawny), so representing mood through color lies within our aim of creating a music visualizer that enhances the user’s listening experience.

The greatest challenge this task posed us was finding out frequency patterns for particular moods, and we could not come up with a reasonable conclusion from the data we had gathered. We were, however, able to build a foundation on tempo and tonality, and approximate the target moods through a combination of these elements. The findings on the effect of tempo on mood are not consistent throughout the papers we have examined(Husain, 2002 and Tsang), thus we added a level of arbitration to represent moods through tempo to achieve parallelism with the moods considered in Bresin. We picked four moods among the ones mentioned in Bresin's table of colors: sadness, happiness, love and anger. None of the given moods had a significant correlation with saturation and value, except for anger which had an observed negative correlation with saturation - we decided not to implement this correlation due to aesthetic concerns. We did, however, correlate tempo and tonality with value according to the conclusions reached by Tsang and Bresin respectively.

Tonality is determined by a composition of functions supplied by librosa - the overall amplitudes for each pitch are compared to reference values mentioned in Gómez. The current version of A-Sharp only supports detection of major and harmonic minor tonality, due to Gómez's analyses being based on classical pieces. Since songs may vary significantly from the aforementioned reference values, the algorithm we apply is by no means perfectly accurate, but it is always able to figure out the key of the song and the tonality with minimal deviation from the correct result in terms of fifths, given that the input song does not feature modulations.

After the base color is determined through this process, the color is shifted with each update according to a combination of cosine functions, allowing it to remain within a reasonable interval of the color spectrum, not deviating greatly from the base color. We added this alteration phase in order to make our visualizer more appealing visually. The periods of the cosine functions are ideally synchronized with the beats of the song, however, librosa's beat tracking functions often pull in weaker beats into the process as well, leading to rapid changes in the period of the cosine function.


The sphere's vertex displacement map is dynamically created from the frequency amplitudes at the current frame and is passed to the shader during runtime. The verticies along each of the latitudes of the sphere respond to the values from their corresponding frequency range, as given by the mapping from \code{analysis.py}. Latitudes higher on the sphere respind to higher frequencies, while rings lower down react to lower pitches.

\section{Results}

Conclusion

fallbacks

improvements

\section{Bibliography}

\begin{itemize}
    \item Tsang, Tawny, and Karen B. Schloss. Associations between Color and Music Are Mediated by Emotion and Influenced by Tempo (n.d.): n. pag. University of California, Berkeley. Web. 12 Dec. 2014.
    \item Gómez, Emilia, and Perfecto Herrera. "Estimating the Tonality of Polyphonic Audio Files: Cognitive Versus Machine Learning Modelling Strategies." (n.d.): n. pag. Web. 12 Dec. 2014.
    \item Bresin, Roberto. "What Is the Color of That Music Performance?" KTH - Royal Institute of Technology, n.d. Web. 12 Dec. 2014.
    \item Husain, Gabriela, William F. Thompson, and Glenn Schellenberg. "Effects of Musical Tempo and Mode on Arousal, Mood, and Spatial Abilities." University of Toronto, n.d. Web. 13 Dec. 2014.
\end{itemize}

\end{document}
