\documentclass{article}
\usepackage[top=1.2in, bottom=1in, left=2in, right=2in]{geometry}
\usepackage{enumerate, multicol}
\usepackage{amsmath}
\usepackage{parskip}
\usepackage{hyperref}
\usepackage[superscript,biblabel]{cite}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\code}{\texttt}

\begin{document}

\begin{titlepage}
\begin{center}

\textsc{\LARGE Cornell University}\\[1.5cm]

\textsc{\Large CS 4621 Practicum Final Report}\\[0.5cm]

% Title
\HRule \\[0.4cm]
{ \huge \bfseries A$\sharp$ -- Music Visualizer \\[0.4cm] }

\HRule \\[1.5cm]

% Group members
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
Shane \textsc{Moore} \\
\emph{swm85}
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
Zachary \textsc{Zimmerman} \\
\emph{ztz3}
\end{flushright}
\end{minipage}
\par\vspace{0.5cm}
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
Emre \textsc{Findik} \\
\emph{ef343}
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
Joseph \textsc{Vinegrad} \\
\emph{jav86}
\end{flushright}
\end{minipage}

\vfill

% Bottom of the page
{\large December 15, 2014}

\end{center}

\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Summary}

A$\sharp$ (A Sharp) is a music visualizer designed for meaningful sound information conveyance. While other visualizers have flashy and intricate animations that pleasantly accompany the music, they do not successfully convey, interperet, or even begin to replace the music. This is an enormous shortcoming of music visualizers --- current applications certainly do not meet the standards of a music visualizer in the true sense of the term. A$\sharp$ attemps to make strides toward filling that gap, with hopes that other visualizers may follow suit.

A$\sharp$ models a song using a single sphere mesh. The sphere is animated through sets of transformations based on comprehensive data analysis of the song's sound file. The application analyzes several important features of a song, including the overall key, beat event times, and the frequency amplitude spectrum at each instant of the song. The results of the song analysis determines the appearance of the sphere during the playback animation.

The source code for this project, as well as any extensions to it, can be found at \url{https://github.com/Oneman2feet/a-sharp/}.

\section{Conceptual Mapping}

In order to achieve an intuitive representation of the chosen song, the conceptual mapping of A$\sharp$'s visuals was designed with great care, as follows:

\begin{itemize}
    \item Overall volume determines sphere size
    \item Beats are shown as pulses in sphere size
    \item Overall pitch controlls sphere elevation (higher up means higher pitch)
    \item Sound complexity (distribution of frequencies) determines sphere distortion
    \item Mood is conveyed by the color of the sphere
\end{itemize}

\subsection{Sound Complexity}

Arguably the most important component in song identification is the melodic contour (i.e. the shape of the melody). Rather than abstracting away the frequencies in a given musical piece, we found that the simplest and most relevant way to display melodic contour was to display the frequency distribution directly.

This was accomplished by mapping each latitudinal band of vertices on the sphere mesh to a ``frequency bin'', or a group of similar frequencies. Each band is then distended from the sphere based on the power (log amplitude) of its resective bin at that time frame. This gives the desired illusion of the bands following the melodic contour of the song as it's being played, allowing individual melodies to be picked out by the user.

\subsection{Mood}

The decision to correlate the mood of a song with the object's color is partly inspired by condition known as synesthesia, in which signals from a particular sense (smell, sight etc.) invoke perceptions in another sense. It has also been found that those without synesthesia may associate sounds with colors \cite{tsang}, though they would not directly perceive them.

The greatest challenge is procedurally quantifying the mood and assigning it a color, for which there is no clear answer. Attempting to discover frequency patterns for particular moods did not seem conclusive, however, approximating the mood based on tempo and tonality brought reasonable results. The findings on the effect of tempo on mood are not consistent throughout (Husain \cite{husain} and Tsang \cite{tsang}), thus a level of arbitration is needed to represent moods through tempo to achieve parallelism with the moods considered in Bresin \cite{bresin}.

To simplify the mood detection algorithm for more reliable results, four moods were picked from among the ones mentioned in Bresin's table of colors: sadness, happiness, love and anger. None of the given moods had a significant correlation with color saturation and value, except for anger which had an observed negative correlation with saturation --- this correlation was decidedly not implemented due to aesthetic concerns. However, both tempo and tonality were correlated with color value according to the conclusions reached by Tsang and Bresin respectively.

\section{Sound Analysis}

Sound analysis takes place as a pre-processing stage, before the visualization is run. The sound data is collected in the module \code{analysis.py}. Much of the waveform analysis is done with the help of the audio and music processing library \href{https://github.com/bmcfee/librosa/}{LibROSA}.

The data received from LibROSA includes:

\begin{itemize}
    \item The uncompressed waveform
    \item Separated harmonic and percussive waveforms
    \item Beat frames (list of time frames for which a beat event occurs)
    \item Mel Spectrogram (amplitude of each frequency bin over time)
    \item Chromagram (amplitude of each pitch-class over time)
\end{itemize}

From this initial data collection, more information about the song is gleaned. The frequency spectrum is truncated to an audible range. The spectrum is also condensed, via a weighted average, into a measure of the overall pitch at each instant. The overall key of the song is statistically predicted based on the prevalences of each pitch-class found in the chromagram, as discussed in \url{http://ismir2004.ismir.net/proceedings/p018-page-92-paper164.pdf}. From this key and the tempo, a base color value is chosen, corresponding to the mood of the entire piece.

The final conclusions of the analysis are then formatted into a python dictionary for use in the graphics module.

\section{Graphics Implementation}

The results from \code{analysis.py} are recieved by the module \code{graphics.py}. This module controls playback of the visualization with the help of the windowing and multimedia library \href{http://www.pyglet.org/}{Pyglet}.

The graphics module uses vertex and fragment shaders to set the sphere mesh's positional and color data. After initialization of the window and global variables and constants, the update function is scheduled to be called regularly by the Pyglet app. At each frame, the sphere's radius, position, color, and displacement map are calculated according to the current sound data. This information is passed into the shaders, \code{DispMapped.vert} and \code{DispMapped.frag}, which update the appearance of the sphere on screen.

    \subsection{Volume and Beats}
    The radius of the sphere is calculated as a sum of contributions
    $$\text{radius} = \text{minimum radius} + \text{current volume} + \text{proximity to beat}$$
    Where the proximity to the nearest beat is expressed as
    $$\left( \frac{\text{time between beats}}{2} - \text{time since previous beat} \right)^2$$
    in order to have the attack and decay of the beat appear as a pulse.

    \subsection{Frequency Analysis}
    At each frame, we programmatically generate a new displacement map for our sphere. The map distends each latitudinal band of the sphere based on the power of its respective frequency bin, as explained in section 2.1

    \subsection{Average Pitch}
    The vertical positioning of the sphere is affected by the overall pitch as determined in \code{analysis.py}. As the average pitch grows higher in frequency, the sphere tends to move upwards, while lower pitches cause it to descend. The vertical motion of the sphere is modeled after a damped harmonic oscillator, where the equilibrium location changes depending on the average pitch frequency.

    \subsection{Mood}
    The color of the sphere at a given moment is determined by the base color found in \code{analysis.py} as well as the position in the song. The rgb values of the sphere's color are shifted according to a sinusoidal function of the form
    $$\cos 2 \pi \left( \frac{1}{2} + \frac{\text{current time}}{32 \cdot \text{length of a beat}} \right)$$
    in order to achieve one full oscillation every 32 beats.

\section{Results}

A$\sharp$

Conclusion

fallbacks

improvements

The tonality is determined by a composition of functions supplied by librosa - the overall amplitudes for each pitch are compared to reference values mentioned in Gómez. The current version of A-Sharp only supports detection of major and harmonic minor tonality, due to Gómez's analyses being based on classical pieces. Since songs may vary significantly from the aforementioned reference values, the algorithm we apply is by no means perfectly accurate, but it is always able to figure out the key of the song and the tonality with minimal deviation from the correct result in terms of fifths, given that the input song does not feature modulations.

After the base color is determined through this process, the color is shifted with each update according to a combination of cosine functions, allowing it to remain within a reasonable interval of the color spectrum, not deviating greatly from the base color. We added this alteration phase in order to make our visualizer more appealing visually. The periods of the cosine functions are ideally synchronized with the beats of the song, however, librosa's beat tracking functions often pull in weaker beats into the process as well, leading to rapid changes in the period of the cosine function.

\begin{thebibliography}{9}

\bibitem{bresin}
    Bresin, Roberto.
    ``What Is the Color of That Music Performance?''
    KTH - Royal Institute of Technology, n.d. Web. 12 Dec. 2014.

\bibitem{gomez}
    G\'{o}mez, Emilia, and Perfecto Herrera.
    ``Estimating the Tonality of Polyphonic Audio Files: Cognitive Versus Machine Learning Modelling Strategies.''
    (n.d.): n. pag. Web. 12 Dec. 2014.

\bibitem{husain}
    Husain, Gabriela, William F. Thompson, and Glenn Schellenberg.
    ``Effects of Musical Tempo and Mode on Arousal, Mood, and Spatial Abilities.''
    University of Toronto, n.d. Web. 13 Dec. 2014.

\bibitem{tsang}
    Tsang, Tawny, and Karen B. Schloss.
    \emph{Associations between Color and Music Are Mediated by Emotion and Influenced by Tempo.}
    (n.d.): n. pag.
    University of California, Berkeley. Web. 12 Dec. 2014.

\end{thebibliography}

\end{document}
