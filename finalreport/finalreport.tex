\documentclass{article}
\usepackage[top=1.2in, bottom=1in, left=2in, right=2in]{geometry}
\usepackage{enumerate, multicol}
\usepackage{amsmath}
\usepackage{parskip}
\usepackage{hyperref}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\code}{\texttt}

\begin{document}

\begin{titlepage}
\begin{center}

\textsc{\LARGE Cornell University}\\[1.5cm]

\textsc{\Large CS 4621 Practicum Final Report}\\[0.5cm]

% Title
\HRule \\[0.4cm]
{ \huge \bfseries A$\sharp$ -- Music Visualizer \\[0.4cm] }

\HRule \\[1.5cm]

% Group members
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
Shane \textsc{Moore} \\
\emph{swm85}
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
Zachary \textsc{Zimmerman} \\
\emph{ztz3}
\end{flushright}
\end{minipage}
\par\vspace{0.5cm}
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
Emre \textsc{Findik} \\
\emph{ef343}
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
Joseph \textsc{Vinegrad} \\
\emph{jav86}
\end{flushright}
\end{minipage}

\vfill

% Bottom of the page
{\large December 15, 2014}

\end{center}

\end{titlepage}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Summary}

A$\sharp$ (A Sharp) is a music visualizer designed for meaningful sound information conveyance. While other visualizers have flashy and intricate animations that pleasantly accompany the music, they do not successfully convey, interperet, or even begin to replace the music. This is an enormous shortcoming of music visualizers --- current applications certainly do not meet the standards of a music visualizer in the true sense of the term. A$\sharp$ attemps to make strides toward filling that gap, with hopes that other visualizers may follow suit.

A$\sharp$ models a song using a single sphere mesh. The sphere is animated through sets of transformations based on comprehensive data analysis of the song's sound file. The application analyzes several important features of a song, including the overall key, beat event times, and the frequency amplitude spectrum at each instant of the song. The results of the song analysis determines the appearance of the sphere during the playback animation.

The source code for this project, as well as any extensions to it, can be found at \url{https://github.com/Oneman2feet/a-sharp/}.

\section{Conceptual Mapping}

In order to achieve an intuitive representation of the chosen song, the conceptual mapping of A$\sharp$'s visuals was designed with great care, as follows:

\begin{itemize}
    \item Overall volume determines object size
    \item Beats are shown as pulses in object size
    \item Overall pitch controlls object elevation (higher up means higher pitch)
    \item Sound complexity (distribution of frequencies) determines object shape
    \item Mood is conveyed by the color of the object
\end{itemize}

\section{Sound Analysis}

Sound analysis takes place as a pre-processing stage, before the visualization is run. The sound data is collected in the module \code{analysis.py}. Much of the waveform analysis is done with the help of the audio and music processing library \href{https://github.com/bmcfee/librosa/}{LibROSA}.

The data received from LibROSA includes:

\begin{itemize}
    \item The uncompressed waveform
    \item Separated harmonic and percussive waveforms
    \item Beat frames (list of time frames for which a beat event occurs)
    \item Mel Spectrogram (amplitude of each frequency bin over time)
    \item Chromagram (amplitude of each pitch-class over time)
\end{itemize}

From this initial data collection, more information about the song is gleaned. The frequency spectrum is truncated to a reasonable range and formatted as a displacement texture for the sphere. The spectrum is also condensed, via a weighted average, into a measure of the overall pitch at each instant. The overall key of the song is statistically predicted based on the prevalences of each pitch-class found in the chromagram, as discussed in \url{http://ismir2004.ismir.net/proceedings/p018-page-92-paper164.pdf}. 

The final conclusions of the analysis are then formatted into a python dictionary for use in the graphics module.

\section{Graphics Implementation}

Once we receive the sound data from analysis.py, we pass them along to our graphics module in \code{app/graphics.py}. First we setup our screen and initialize constants and data structures based on the sound data. At each frame, we update features of the sphere, including radius, color, vertical positon, and displacement map. This information is passed into the shaders, \code{DispMapped.vert} and \code{DispMapped.frag}, which update the appearance of the sphere on screen. 

physically based animation spring model

displacement mapping

contributions to radius

color cosine function for color scheme, synced with beats

For our graphics pipeline, we wrote vertex and fragment shaders for our sphere to set positional and color data. After receiving data for each frame from the sound analysis, we set the radius of the sphere according to beat pulse and sound amplitude. Then, we use frequency data to compute vertical translations of the sphere. The idea behind this is to create an upward movement of the sphere when the pitch rises and a downward movement when pitch falls. We model this vertical motion of the sphere using a damped harmonic oscillator. 

We also use frequency amplitude data to set displacement magnitudes. We dynamically create a texture map using this data at each frame. Essentially, the objective here is to displace higher points on the sphere according to the amplitudes of high frequencies, and lower points on the sphere according to the amplitudes of low frequencies. Finally we set shader uniforms based on the computed radius and texture map at each frame.

\section{Results}

Conclusion

\end{document}
